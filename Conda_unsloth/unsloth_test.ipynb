{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d467d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conda 23.7.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while loading conda entry point: anaconda-cloud-auth (cannot import name 'ChannelAuthBase' from 'conda.plugins.types' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\conda\\plugins\\types.py))\n",
      "Error while loading conda entry point: anaconda-cloud-auth (cannot import name 'ChannelAuthBase' from 'conda.plugins.types' (C:\\ProgramData\\anaconda3\\Lib\\site-packages\\conda\\plugins\\types.py))\n"
     ]
    }
   ],
   "source": [
    "!Conda -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9eb88b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.4.0\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp311-cp311-win_amd64.whl (2442.0 MB)\n",
      "Collecting filelock (from torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0) (4.12.2)\n",
      "Collecting sympy (from torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting jinja2 (from torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch==2.4.0)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.4.0+cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4e58ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to c:\\users\\gostj\\appdata\\local\\temp\\pip-install-rrtudq85\\unsloth_676791e0e8fa48a6bb2f6b80ea2e9c17\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 7476d4b5f68bb11f9f7841df4c08baf7a9e8f632\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting transformers>=4.43.2 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets>=2.16.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting tqdm (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.0)\n",
      "Collecting wheel>=0.42.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached numpy-2.1.1-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Collecting huggingface-hub (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached hf_transfer-0.1.8-cp311-none-win_amd64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pyarrow-17.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiohttp-3.10.5-cp311-cp311-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tqdm->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached regex-2024.7.24-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.43.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached rich-13.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached yarl-1.10.0-cp311-cp311-win_amd64.whl.metadata (46 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Using cached numpy-2.1.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Using cached sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Using cached hf_transfer-0.1.8-cp311-none-win_amd64.whl (1.2 MB)\n",
      "Using cached tyro-0.8.10-py3-none-any.whl (105 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached aiohttp-3.10.5-cp311-cp311-win_amd64.whl (379 kB)\n",
      "Using cached pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached regex-2024.7.24-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.8.0-py3-none-any.whl (241 kB)\n",
      "Using cached safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Using cached idna-3.8-py3-none-any.whl (66 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.10.0-cp311-cp311-win_amd64.whl (110 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml): started\n",
      "  Building wheel for unsloth (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unsloth: filename=unsloth-2024.8-py3-none-any.whl size=153847 sha256=96de7a09ca37e8e0f068397187f318251ad67da35848d250f55f78c185692aa6\n",
      "  Stored in directory: C:\\Users\\gostj\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-_nia_7jy\\wheels\\d1\\17\\05\\850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: sentencepiece, pytz, xxhash, wheel, urllib3, unsloth, tzdata, tqdm, shtab, safetensors, regex, pyyaml, protobuf, numpy, multidict, mdurl, idna, hf-transfer, frozenlist, docstring-parser, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pyarrow, pandas, multiprocess, markdown-it-py, aiosignal, rich, huggingface-hub, aiohttp, tyro, tokenizers, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.3.2 datasets-2.21.0 dill-0.3.8 docstring-parser-0.16 frozenlist-1.4.1 hf-transfer-0.1.8 huggingface-hub-0.24.6 idna-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 numpy-2.1.1 pandas-2.2.2 protobuf-3.20.3 pyarrow-17.0.0 pytz-2024.1 pyyaml-6.0.2 regex-2024.7.24 requests-2.32.3 rich-13.8.0 safetensors-0.4.5 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 tyro-0.8.10 tzdata-2024.1 unsloth-2024.8 urllib3-2.2.2 wheel-0.44.0 xxhash-3.5.0 yarl-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git 'C:\\Users\\gostj\\AppData\\Local\\Temp\\pip-install-rrtudq85\\unsloth_676791e0e8fa48a6bb2f6b80ea2e9c17'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963840ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5091df9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\n",
      "  Using cached xformers-0.0.27.post2-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from xformers) (2.1.1)\n",
      "Requirement already satisfied: torch==2.4.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from xformers) (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0->xformers) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0->xformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0->xformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0->xformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0->xformers) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch==2.4.0->xformers) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from jinja2->torch==2.4.0->xformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from sympy->torch==2.4.0->xformers) (1.3.0)\n",
      "Using cached xformers-0.0.27.post2-cp311-cp311-win_amd64.whl (89.0 MB)\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.27.post2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e6ee297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from trl) (2.4.0+cu121)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from trl) (4.44.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from trl) (2.1.1)\n",
      "Collecting accelerate (from trl)\n",
      "  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: datasets in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from trl) (2.21.0)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from trl) (0.8.10)\n",
      "Requirement already satisfied: filelock in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.24.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (4.66.5)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (13.8.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate->trl) (6.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets->trl) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets->trl) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from datasets->trl) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (1.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from pandas->datasets->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Using cached trl-0.10.1-py3-none-any.whl (280 kB)\n",
      "Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "Installing collected packages: accelerate, trl\n",
      "Successfully installed accelerate-0.34.2 trl-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef782e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Using cached peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (2.4.0+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from peft) (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers->peft) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Using cached peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a70d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (0.34.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (2.4.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (0.24.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b0fde41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from bitsandbytes) (2.4.0+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from torch->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gostj\\desktop\\ai_nextgen\\llama3-fine-tuning\\.venv\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl (136.5 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71705681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement triton==3.0.0 (from versions: none)\n",
      "ERROR: No matching distribution found for triton==3.0.0\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install triton==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5144782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xformers\n",
      "  Obtaining dependency information for xformers from https://files.pythonhosted.org/packages/1c/0f/095c4cbcb099c560f6145747c79b4293e85625e5bfa554dddf5a23932d0b/xformers-0.0.27.post2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xformers-0.0.27.post2-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: trl in c:\\users\\gostj\\appdata\\roaming\\python\\python311\\site-packages (0.7.11)\n",
      "Requirement already satisfied: peft in c:\\users\\gostj\\appdata\\roaming\\python\\python311\\site-packages (0.9.0)\n",
      "Requirement already satisfied: accelerate in c:\\programdata\\anaconda3\\lib\\site-packages (0.27.2)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\gostj\\appdata\\roaming\\python\\python311\\site-packages (0.43.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
      "ERROR: No matching distribution found for triton\n"
     ]
    }
   ],
   "source": [
    "# xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13244eef",
   "metadata": {},
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "621dadd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/61.0 kB 320.0 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/61.0 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 61.0/61.0 kB 540.4 kB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/15.8 MB 4.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.8/15.8 MB 8.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.5/15.8 MB 10.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.4/15.8 MB 12.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.3/15.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.2/15.8 MB 15.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 6.0/15.8 MB 15.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.7/15.8 MB 16.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.8/15.8 MB 17.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.9/15.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.9/15.8 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.5/15.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.1/15.8 MB 15.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.4/15.8 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 18.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.3/15.8 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.3/15.8 MB 18.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 16.8 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.1\n",
      "    Uninstalling numpy-2.1.1:\n",
      "      Successfully uninstalled numpy-2.1.1\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28489ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            VersionNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\gostj\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------ ------------\n",
      "accelerate         0.34.2\n",
      "aiohappyeyeballs   2.4.0\n",
      "aiohttp            3.10.5\n",
      "aiosignal          1.3.1\n",
      "asttokens          2.4.1\n",
      "attrs              24.2.0\n",
      "bitsandbytes       0.43.3\n",
      "certifi            2024.8.30\n",
      "charset-normalizer 3.3.2\n",
      "colorama           0.4.6\n",
      "comm               0.2.2\n",
      "datasets           2.21.0\n",
      "debugpy            1.8.5\n",
      "decorator          5.1.1\n",
      "dill               0.3.8\n",
      "docstring_parser   0.16\n",
      "executing          2.1.0\n",
      "filelock           3.13.1\n",
      "frozenlist         1.4.1\n",
      "fsspec             2024.2.0\n",
      "hf_transfer        0.1.8\n",
      "huggingface-hub    0.24.6\n",
      "idna               3.8\n",
      "ipykernel          6.29.5\n",
      "ipython            8.27.0\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.3\n",
      "jupyter_client     8.6.2\n",
      "jupyter_core       5.7.2\n",
      "markdown-it-py     3.0.0\n",
      "MarkupSafe         2.1.5\n",
      "matplotlib-inline  0.1.7\n",
      "mdurl              0.1.2\n",
      "mpmath             1.3.0\n",
      "multidict          6.0.5\n",
      "multiprocess       0.70.16\n",
      "nest-asyncio       1.6.0\n",
      "networkx           3.2.1\n",
      "numpy              1.26.4\n",
      "packaging          24.1\n",
      "pandas             2.2.2\n",
      "parso              0.8.4\n",
      "peft               0.12.0\n",
      "pip                24.0\n",
      "platformdirs       4.3.2\n",
      "prompt_toolkit     3.0.47\n",
      "protobuf           3.20.3\n",
      "psutil             6.0.0\n",
      "pure_eval          0.2.3\n",
      "pyarrow            17.0.0\n",
      "Pygments           2.18.0\n",
      "python-dateutil    2.9.0.post0\n",
      "pytz               2024.1\n",
      "pywin32            306\n",
      "PyYAML             6.0.2\n",
      "pyzmq              26.2.0\n",
      "regex              2024.7.24\n",
      "requests           2.32.3\n",
      "rich               13.8.0\n",
      "safetensors        0.4.5\n",
      "sentencepiece      0.2.0\n",
      "setuptools         65.5.0\n",
      "shtab              1.7.1\n",
      "six                1.16.0\n",
      "stack-data         0.6.3\n",
      "sympy              1.12\n",
      "tokenizers         0.19.1\n",
      "torch              2.4.0+cu121\n",
      "tornado            6.4.1\n",
      "tqdm               4.66.5\n",
      "traitlets          5.14.3\n",
      "transformers       4.44.2\n",
      "triton             2.1.0\n",
      "trl                0.10.1\n",
      "typing_extensions  4.12.2\n",
      "tyro               0.8.10\n",
      "tzdata             2024.1\n",
      "unsloth            2024.8\n",
      "urllib3            2.2.2\n",
      "wcwidth            0.2.13\n",
      "wheel              0.44.0\n",
      "xformers           0.0.27.post2\n",
      "xxhash             3.5.0\n",
      "yarl               1.10.0\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "336e38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel \n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dec9d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Mistral patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060 Laptop GPU. Max memory: 6.0 GB. Platform = Windows.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MistralForCausalLM.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m fourbit_models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-v0.3-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,      \u001b[38;5;66;03m# New Mistral v3 2x faster!\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-instruct-v0.3-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/gemma-7b-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,             \u001b[38;5;66;03m# Gemma 2.2x faster!\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ] \u001b[38;5;66;03m# More models at https://huggingface.co/unsloth\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/mistral-7b-v0.3-bnb-4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit_fp32_cpu_offload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\unsloth\\models\\loader.py:301\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[1;32m~\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\unsloth\\models\\mistral.py:346\u001b[0m, in \u001b[0;36mFastMistralModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m    333\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/mistral-7b-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    345\u001b[0m ):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFastMistralModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\unsloth\\models\\llama.py:1580\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[38;5;66;03m# Cannot be None, since HF now checks for the config\u001b[39;00m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[1;32m-> 1580\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[0;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;66;03m# Return old flag\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_HUB_ENABLE_HF_TRANSFER\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m old_hf_transfer\n",
      "File \u001b[1;32m~\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\Desktop\\AI_nextgen\\Llama3-fine-tuning\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3832\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3826\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[0;32m   3827\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[0;32m   3828\u001b[0m )\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m   3831\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3834\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   3835\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[1;31mTypeError\u001b[0m: MistralForCausalLM.__init__() got an unexpected keyword argument 'load_in_8bit_fp32_cpu_offload'"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048 # Supports RoPE Scaling interally, so choose any!\n",
    "# Get LAION dataset\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train\")\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    "#     load_in_8bit_fp32_cpu_offload=True,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 60,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3119n",
   "language": "python",
   "name": "python3119n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
